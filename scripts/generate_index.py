import os
import glob
import frontmatter
import re

def extract_summary(content, length=100):
    """
    è¨˜äº‹ã®å†’é ­ã‹ã‚‰è¦ç´„ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    HTMLã‚³ãƒ¡ãƒ³ãƒˆã€è¦‹å‡ºã—ã€ç”»åƒãªã©ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ã€‚
    """
    # ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã¯ frontmatter.load ã§æ—¢ã«é™¤å»ã•ã‚Œã¦ã„ã‚‹å‰æ
    
    lines = content.split('\n')
    summary_text = ""
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        if line.startswith('#'): # è¦‹å‡ºã—
            continue
        if line.startswith('!['): # ç”»åƒ
            continue
        if line.startswith('<img'): # imgã‚¿ã‚°
            continue
        if line.startswith('::: '): # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹
            continue
        if line.startswith(':::'): # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ–ãƒ­ãƒƒã‚¯çµ‚äº†
            continue
        if line.startswith('```'): # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
            continue
            
        # ã“ã‚Œã‚‰ä»¥å¤–ã®è¡Œã‚’çµåˆ
        summary_text += line + " "
        
        if len(summary_text) > length:
            break
            
    # Markdownè¨˜æ³•ã®é™¤å» (ç°¡æ˜“ç‰ˆ)
    summary_text = re.sub(r'!\[.*?\]\(.*?\)', '', summary_text) # ç”»åƒãƒªãƒ³ã‚¯é™¤å»
    summary_text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', summary_text) # ãƒªãƒ³ã‚¯é™¤å»
    summary_text = re.sub(r'\*\*(.*?)\*\*', r'\1', summary_text) # å¤ªå­—
    
    return summary_text[:length] + "..." if len(summary_text) > length else summary_text

def generate_index(articles_dir, output_file):
    files = glob.glob(os.path.join(articles_dir, "*.md"))
    files.sort(key=os.path.getmtime, reverse=True) # æ–°ã—ã„é †
    
    header = """# è¨˜äº‹ä¸€è¦§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

å…¨è¨˜äº‹ã®æ¦‚è¦ä¸€è¦§ã§ã™ã€‚`python3 .agents/scripts/generate_index.py` ã§æ›´æ–°ã§ãã¾ã™ã€‚

| ã‚¿ã‚¤ãƒˆãƒ« | Type | Topics | æ¦‚è¦ |
| --- | --- | --- | --- |
"""
    
    rows = []
    for file_path in files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                post = frontmatter.load(f)
                
            metadata = post.metadata
            title = metadata.get('title', 'No Title')
            emoji = metadata.get('emoji', 'ğŸ“„')
            type_ = metadata.get('type', '-')
            topics = metadata.get('topics', [])
            published = metadata.get('published', True)
            
            if not published:
                title = f"[ä¸‹æ›¸ã] {title}"
            
            # ç›¸å¯¾ãƒ‘ã‚¹è¨ˆç®—
            rel_path = os.path.relpath(file_path, os.path.dirname(output_file))
            
            # Topicsæ•´å½¢
            topics_str = ", ".join([f"`{t}`" for t in topics])
            
            # æ¦‚è¦æŠ½å‡º
            summary = extract_summary(post.content)
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œä½œæˆ
            row = f"| {emoji} [{title}]({rel_path}) | {type_} | {topics_str} | {summary} |"
            rows.append(row)
            
        except Exception as e:
            print(f"Error processing {file_path}: {e}")
            
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(header + "\n".join(rows))
    
    print(f"Generated index at {output_file} ({len(rows)} articles)")

if __name__ == "__main__":
    target_dir = "articles"
    output_path = ".agents/docs/article_index.md"
    
    if os.path.exists(target_dir):
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã¯ä½œæˆ
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        generate_index(target_dir, output_path)
    else:
        print(f"Directory not found: {target_dir}")
